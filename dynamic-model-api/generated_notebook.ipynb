{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cb066",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary dependencies\n",
    "%pip install torch numpy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89162498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATALOADERS = {\n",
    "    \"pima\": {\n",
    "        \"X\": pd.read_csv(\"datasets/pima-indians-diabetes.csv\").iloc[:, :-1].values,\n",
    "        \"y\": pd.read_csv(\"datasets/pima-indians-diabetes.csv\").iloc[:, -1].values,\n",
    "    },\n",
    "    \"MNIST\": {\n",
    "        \"train\": datasets.MNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "        \"test\": datasets.MNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "    },\n",
    "    \"FashionMNIST\": {\n",
    "        \"train\": datasets.FashionMNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "        \"test\": datasets.FashionMNIST(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "    },\n",
    "    \"CIFAR10\": {\n",
    "        \"train\": datasets.CIFAR10(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "        \"test\": datasets.CIFAR10(\n",
    "            root=\"data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "ACTIVATIONS = {\n",
    "    \"ReLU\": nn.ReLU(),\n",
    "    \"Sigmoid\": nn.Sigmoid(),\n",
    "    \"Tanh\": nn.Tanh(),\n",
    "    \"Softmax\": nn.Softmax(),\n",
    "    \"LeakyReLU\": nn.LeakyReLU(),\n",
    "    \"PReLU\": nn.PReLU(),\n",
    "}\n",
    "\n",
    "\n",
    "LAYERS = {\n",
    "    \"Flatten\": nn.Flatten(), # no argumnets for Flatten()\n",
    "    \"Linear\": lambda i, o: nn.Linear(i, o),\n",
    "    \"Conv2D\": lambda i, o, k_size: nn.Conv2d(i, o, k_size), # i = input channels (1 --> grayscale, 3 --> RGB), o = output channels (number of filters), k_size = kernel size\n",
    "    \"Conv1D\": lambda i, o, k_size: nn.Conv1d(i, o, k_size),\n",
    "    \"Conv3D\": lambda i, o, k_size: nn.Conv3d(i, o, k_size),\n",
    "    \"LSTM\": lambda i, h_size: nn.LSTM(i, h_size),\n",
    "    \"GRU\": lambda i, h_size: nn.GRU(i, h_size),\n",
    "    \"RNN\": lambda i, h_size: nn.RNN(i, h_size),\n",
    "}\n",
    "\n",
    "\n",
    "LOSSES = {\n",
    "    \"BCE\": nn.BCELoss(),  # binary cross entropy --> 0 or 1 classification models\n",
    "    \"CrossEntropy\": nn.CrossEntropyLoss(),  # multi-class classification models\n",
    "    # \"MSE\": nn.MSELoss() # regression models\n",
    "}\n",
    "\n",
    "\n",
    "OPTIMIZERS = {\n",
    "    \"Adam\": lambda model_params, lr: optim.Adam(\n",
    "        model_params, lr\n",
    "    ),  # momentum parameter is optional\n",
    "    \"AdamW\": lambda model_params, lr: optim.AdamW(model_params, lr),\n",
    "    \"SGD\": lambda model_params, lr: optim.SGD(model_params, lr),\n",
    "    \"RMSprop\": lambda model_params, lr: optim.RMSprop(model_params, lr),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c2dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        layers_list = [\n",
    "            nn.Linear(8, 12),nn.ReLU(),nn.Linear(12, 8),nn.ReLU(),nn.Linear(8, 1),nn.Sigmoid(),\n",
    "        ]\n",
    "        self.layers = nn.ModuleList(layers_list)\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd06577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, model, input, loss, optimizer, batch_size):\n",
    "        # validate data inputs before this point, each valid key should exist and errors for invalid\n",
    "        self.input = input\n",
    "        ds = DATALOADERS[\"pima\"]\n",
    "\n",
    "        self.device = (  # for GPU access --> works with CPU as well\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "\n",
    "        # MOVE MODEL TO DEVICE\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # preprocessing data here!!!\n",
    "        if input == \"pima\":\n",
    "            X = ds[\"X\"]\n",
    "            y = ds[\"y\"]\n",
    "\n",
    "            # split test and training data using ski-kit learn module\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            # could normalize the data here\n",
    "            # create tensors\n",
    "            X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train_tensor = torch.tensor(y_train, dtype=torch.float32).reshape(\n",
    "                -1, 1\n",
    "            )  # Reshape for binary classification\n",
    "            X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "            y_test_tensor = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "            # create dataset objects\n",
    "            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "            test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "            # create dataLoader objects\n",
    "            self.train_loader = DataLoader(\n",
    "                train_dataset, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "            self.test_loader = DataLoader(\n",
    "                test_dataset, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "        else:\n",
    "            train_set = ds[\n",
    "                \"train\"\n",
    "            ]  # ds[\"train\"] is a dataset object, already transformed into tensor\n",
    "            test_set = ds[\"test\"]\n",
    "            self.train_loader = DataLoader(\n",
    "                train_set, batch_size=batch_size, shuffle=True\n",
    "            )\n",
    "            self.test_loader = DataLoader(\n",
    "                test_set, batch_size=batch_size, shuffle=False\n",
    "            )\n",
    "\n",
    "        self.loss_fn = LOSSES[\"BCE\"]\n",
    "        self.optimizer = OPTIMIZERS[\"Adam\"](\n",
    "            self.model.parameters(), 0.001\n",
    "        )\n",
    "\n",
    "        self.final_loss = -1\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1d3849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "def train(self, n_epochs, batch_size):\n",
    "    # if self.input == \"pima\":\n",
    "    #     # PIMA TRAINING FUNCTION\n",
    "    #     for epoch in range(n_epochs):\n",
    "    #         for i in range(0, len(self.X), batch_size):\n",
    "    #             batchX = self.X[i : i + batch_size]\n",
    "    #             y_pred = self.model(batchX)\n",
    "    #             y_batch = self.Y[i : i + batch_size]\n",
    "    #             loss = self.loss_fn(y_pred, y_batch)\n",
    "    #             self.optimizer.zero_grad()\n",
    "    #             loss.backward()\n",
    "    #             self.optimizer.step()\n",
    "\n",
    "    #             self.final_loss = loss\n",
    "\n",
    "    #     return self.final_loss\n",
    "    # else:\n",
    "    #     # IMAGE DATASETS TRAINING FUNCTION\n",
    "    size = len(self.train_loader.dataset)\n",
    "    # num_batches = len(self.train_loader)\n",
    "    self.model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(self.train_loader):\n",
    "        X, y = X.to(self.device), y.to(self.device)\n",
    "        # Compute prediction error\n",
    "        pred = self.model(X)\n",
    "        loss = self.loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(\n",
    "            pred, 1\n",
    "        )  # Get the predicted class (index with max value)\n",
    "        correct += (predicted == y).sum().item()  # Count correct predictions\n",
    "        total += y.size(0)  # Count total predictions\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_train_loss = train_loss / len(self.train_loader)\n",
    "    # Calculate accuracy as a percentage\n",
    "    avg_acc = 100 * correct / total\n",
    "    return avg_train_loss, avg_acc\n",
    "\n",
    "Train.train = train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, n_epochs, batch_size):\n",
    "    # if self.input == \"pima\":\n",
    "    #     # PIMA TRAINING FUNCTION\n",
    "    #     # do nothing, because pima currently does not have a test_dataset (only has X, y dataset --> has not been split yet)\n",
    "    #     print(\"PIMA is not configured to have a test dataset yet\")\n",
    "    # else:\n",
    "    size = len(self.test_loader.dataset)\n",
    "    num_batches = len(self.test_loader)\n",
    "    self.model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in self.test_loader:\n",
    "            X, y = X.to(self.device), y.to(self.device)\n",
    "            # Compute prediction error\n",
    "            pred = self.model(X)\n",
    "            test_loss += self.loss_fn(pred, y).item()\n",
    "            correct += (\n",
    "                (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            )  # for accuracy\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    avg_acc = 100 * correct\n",
    "\n",
    "    # Print loss & accuracy\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {(avg_acc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "\n",
    "    # Average loss over all batches\n",
    "    avg_test_loss = test_loss / len(self.test_loader)\n",
    "    return avg_test_loss, avg_acc\n",
    "Train.test = test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68231e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_log(self, n_epochs, batch_size):\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "        for t in range(n_epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            avg_train_loss, train_avg_acc = self.train(n_epochs, batch_size)\n",
    "            avg_test_loss, test_avg_acc = self.test(n_epochs, batch_size)\n",
    "\n",
    "            # Store losses\n",
    "            train_losses.append(avg_train_loss)\n",
    "            train_accs.append(train_avg_acc)\n",
    "            test_losses.append(avg_test_loss)\n",
    "            test_accs.append(test_avg_acc)\n",
    "\n",
    "        # calculate average accuracy and average loss\n",
    "        avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "        avg_test_acc = sum(test_accs) / len(test_accs)\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "\n",
    "        print(\"Done!\")\n",
    "\n",
    "        return {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "            \"avg_train_loss\": avg_train_loss,\n",
    "            \"avg_test_loss\": avg_test_loss,\n",
    "            \"avg_train_acc\": avg_train_acc,\n",
    "            \"avg_test_acc\": avg_test_acc,\n",
    "        }\n",
    "\n",
    "Train.train_test_log = train_test_log\n",
    "        # can add more information to this dictionary, like the saved model, best epochs, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ff0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    model = DynamicModel()\n",
    "    # model = DynamicModel(params[\"layers\"]).to(device)\n",
    "    print(model)\n",
    "    t = Train(\n",
    "        model=model,\n",
    "        input=\"pima\",\n",
    "        loss=\"BCE\",\n",
    "        optimizer=\"Adam\",\n",
    "        batch_size=10,\n",
    "    )\n",
    "\n",
    "    RESULTS = t.train_test_log(3, batch_size=10)\n",
    "    print(\"Results:\")\n",
    "    print(\"Average Training Loss: \", RESULTS[\"avg_train_loss\"])\n",
    "    print(\"Average Testing Loss: \", RESULTS[\"avg_test_loss\"])\n",
    "    print(\"Average Training Accuracy: \", RESULTS[\"avg_train_acc\"])\n",
    "    print(\"Average Testing Accuracy: \", RESULTS[\"avg_test_acc\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
